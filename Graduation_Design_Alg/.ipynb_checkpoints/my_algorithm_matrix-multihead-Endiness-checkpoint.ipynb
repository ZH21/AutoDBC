{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff9e374",
   "metadata": {},
   "source": [
    "## 1.提取DBC文件信息转为数据域分割的GroundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c519c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "node = []\n",
    "allDatas = []\n",
    "siganlList = []\n",
    "SignalsName = []\n",
    "messageName = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcc02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file_name):\n",
    "    global node,allDatas,siganlList,SignalsName,messageName\n",
    "    ''' 得到dbc文件的绝对路径'''\n",
    "    filePath = file_name\n",
    "    if filePath:\n",
    "        print(filePath)\n",
    "        f = open(filePath, \"r\")  # 设置文件对象\n",
    "    else:\n",
    "        print(\"读取文件失败！\")\n",
    "        return 0\n",
    "    \"\"\"\n",
    "    NodesPattern:节点\n",
    "    MessagePattern：消息\n",
    "    SignalPattern：信号\n",
    "    \"\"\"\n",
    "    NodesPattern = re.compile(r\"BU_: (.*)\", re.S)\n",
    "    MessagePattern = re.compile(r\"BO_ (.*?) (.*?): (.*?) (.*)\", re.S)\n",
    "#     SignalPattern = re.compile('''SG_ (.*?) : (.*?)\\|(.*?)@.*? \\((.*?),(.*?)\\) \\[(.*?)\\|(.*?)\\] \"(.*?)\" (.*)''', re.S)\n",
    "    SignalPattern = re.compile('''SG_ (.*?) : (.*?)\\|(.*?)@([0-9])([+|-]) \\((.*?),(.*?)\\) \\[(.*?)\\|(.*?)\\] \"(.*?)\" (.*)''', re.S)\n",
    "    DefaultValue = '''BA_ \"GenSig(.*?)\" SG_ (\\d+) signalname (\\d+);'''\n",
    "\n",
    "    line = f.readline()\n",
    "    allDatas=[]\n",
    "    while line:\n",
    "        \"\"\" 匹配出节点 \"\"\"\n",
    "        NodesSearched = re.search(NodesPattern, line.strip())\n",
    "        if NodesSearched:\n",
    "            node = NodesSearched.group(1).split(\" \")\n",
    "            #print(node)\n",
    "        \"\"\" 匹配出消息 \"\"\"\n",
    "        MessageSearched = re.search(MessagePattern, line.strip())\n",
    "        if MessageSearched:\n",
    "            siganlList.clear()\n",
    "            \"\"\"如果匹配到了message，则获取到message的相关参数 \n",
    "             比如匹配到了NM_Message_ESC_409，则会解析出改message的一些参数构成list对象['1033', 'NM_Message_ESC_409', '8', 'ESC']\n",
    "             这四个参数分别是 messgage ID ;message name ; messgae dataLenth ,message sender\n",
    "             而且把这个list对象 加在了 siganlList 索引0的位置\n",
    "            \"\"\"\n",
    "            Message = list(MessageSearched.groups())\n",
    "            siganlList.append(Message)\n",
    "            \"\"\" 只 要 message的名字 messageName 列表中\"\"\"\n",
    "            messageName.append(Message[1])\n",
    "            \"\"\"读取下一行\"\"\"\n",
    "            line = f.readline()\n",
    "            \"\"\"因为有些message并没有定义signal，所以 下一行还是message\"\"\"\n",
    "            MessageSearched = re.search(MessagePattern, line.strip())\n",
    "            SignalSearched = re.search(SignalPattern, line.strip())\n",
    "            \"\"\"下一行如果不是message的内容 就一定是signal的内容了\"\"\"\n",
    "            if not MessageSearched:\n",
    "                while SignalSearched:\n",
    "                    \"\"\"获取信号的参数追加到siganlList\"\"\"\n",
    "                    signal = list(SignalSearched.groups())\n",
    "                    siganlList.append(signal)\n",
    "                    \"\"\"只获取 signal name\"\"\"\n",
    "                    SignalsName.append(signal[0])\n",
    "\n",
    "                    # 再次解析信号，直到这个message下的信号全部解析完毕\n",
    "                    line = f.readline()\n",
    "                    SignalSearched = re.search(SignalPattern, line.strip())\n",
    "           # print(siganlList)\n",
    "            c = copy.deepcopy(siganlList)\n",
    "            allDatas.append(c)\n",
    "        else:\n",
    "            line = f.readline()\n",
    "            MessageSearched = re.search(MessagePattern, line.strip())\n",
    "    f.close()  # 将文件关闭\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ae5975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\cadillac_ct6_object.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\cadillac_ct6_powertrain.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\gm_global_a_powertrain.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\honda_civic_hatchback_ex_2017_can_generated.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\honda_civic_sedan_16_diesel_2019_can_generated.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\honda_civic_touring_2016_can_generated.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\tesla_can.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\tesla_powertrain.dbc',\n",
       " 'D:\\\\--storage--\\\\program\\\\Code\\\\jupyter-notebook\\\\CarNetworkIDS\\\\Code\\\\MyTask\\\\My_DBC\\\\tesla_radar.dbc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_names = []\n",
    "path = os.path.abspath('../My_DBC')\n",
    "# path = os.path.abspath('../opendbc-master/opendbc-master-delFalseData')\n",
    "# path = os.path.abspath('./opendbc-master/opendbc-master')\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "for i in dirs:\n",
    "    if os.path.splitext(i)[1] == \".dbc\":\n",
    "        file_names.append(os.path.join(path,i))\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583de4e",
   "metadata": {},
   "source": [
    "## 2.处理原始bit数据和GroundTruth数据的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702784f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_labels(index):\n",
    "    file_name = file_names[index]\n",
    "    readFile(file_name)\n",
    "\n",
    "    # 将DBC文件中的ID(10进制数)改为ID(16进制数)\n",
    "    data_gts = {}\n",
    "    for alldata in allDatas:\n",
    "        data_gts[str(hex(int(alldata[0][0])))[2:]] = alldata\n",
    "\n",
    "    # 得到DBC文件中各ID Message对应信号位置\n",
    "    all_id_lsbs = {}\n",
    "    for k in data_gts.keys():\n",
    "        message = data_gts[k]\n",
    "        lsb_list = []\n",
    "        series = np.zeros(64)\n",
    "        message_id = message[0][0]+\"_\"+message[0][1]\n",
    "        del message[0]\n",
    "        for signal in message:\n",
    "            start = int(signal[1])\n",
    "            length = int(signal[2])\n",
    "            endianness = int(signal[3])\n",
    "            start_row = int(start / 8)\n",
    "            start_col = 8 - (start - start_row * 8) - 1\n",
    "            new_start = start_row * 8 + start_col\n",
    "            if endianness == 0:\n",
    "                new_end = new_start + length\n",
    "                cur_lsb = new_end - 1\n",
    "            if endianness == 1:\n",
    "                cur_lsb = new_start\n",
    "            lsb_list.append(cur_lsb)\n",
    "        all_id_lsbs[k] = lsb_list\n",
    "\n",
    "    # 将各ID Message的信号位置改为 0/1 label\n",
    "    all_id_labels = {}\n",
    "    for k in all_id_lsbs:\n",
    "        cur_lsb_list = all_id_lsbs[k]\n",
    "        cur_label = np.zeros(64)\n",
    "        for cur_lsb in cur_lsb_list:\n",
    "            cur_label[cur_lsb] = 1\n",
    "        all_id_labels[k] = cur_label\n",
    "    return all_id_lsbs, all_id_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ba49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(id_tracedict_int, all_id_labels):\n",
    "    ids = []\n",
    "    for id in id_tracedict_int.keys():\n",
    "        if(id_tracedict_int.__contains__(id) and all_id_labels.__contains__(id)):\n",
    "            ids.append(id)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bad01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace(data):\n",
    "    group = data.groupby(by=\"ID\")\n",
    "    id_tracedict = {}\n",
    "    for trace in list(group):\n",
    "        id_tracedict[trace[0]] = trace[1][\"bin\"].apply(lambda x: list(x)).apply(lambda x: list(map(int, x))).tolist()\n",
    "    return id_tracedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62034f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 设置全局的随机种子\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110411d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_data_dict(cur_ids, id_tracedict_int, all_id_labels, trace_data_dict, trace_label_dict): \n",
    "    for id in cur_ids:\n",
    "        trace_data_dict[id] = id_tracedict_int[id]\n",
    "        trace_label_dict[id] = all_id_labels[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fd7cd",
   "metadata": {},
   "source": [
    "## 3.综合Honda和Cadillac数据，得到训练集和测试集（包括0/1Label）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c577f",
   "metadata": {},
   "source": [
    "#### 得到label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5fc9282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\honda_civic_hatchback_ex_2017_can_generated.dbc\n",
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\honda_civic_sedan_16_diesel_2019_can_generated.dbc\n",
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\honda_civic_touring_2016_can_generated.dbc\n"
     ]
    }
   ],
   "source": [
    "# 合并所有Honda车的Ground Truth的 0/1 label\n",
    "all_id_lsbs3, dict3 = get_id_labels(3)\n",
    "all_id_lsbs4, dict4 = get_id_labels(4)\n",
    "all_id_lsbs5, dict5 = get_id_labels(5)\n",
    "honda_id_labels = dict()\n",
    "for key in dict3.keys() | dict4.keys() | dict5.keys():\n",
    "    for d in (dict3, dict4, dict5):\n",
    "        if d.__contains__(key):\n",
    "            honda_id_labels[key] = d[key]\n",
    "            break\n",
    "honda_id_lsb = dict()\n",
    "for key in all_id_lsbs3.keys() | all_id_lsbs4.keys() | all_id_lsbs5.keys():\n",
    "    for d in (all_id_lsbs3, all_id_lsbs4, all_id_lsbs5):\n",
    "        if d.__contains__(key):\n",
    "            honda_id_lsb[key] = d[key]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c9a22fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\cadillac_ct6_object.dbc\n",
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\cadillac_ct6_powertrain.dbc\n",
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\gm_global_a_powertrain.dbc\n"
     ]
    }
   ],
   "source": [
    "# 合并所有Cadillac以及Chevy车的Ground Truth的 0/1 label\n",
    "all_id_lsbs0, dict0 = get_id_labels(0)\n",
    "all_id_lsbs1, dict1 = get_id_labels(1)\n",
    "all_id_lsbs2, dict2 = get_id_labels(2)\n",
    "cadillac_id_labels = dict()\n",
    "for key in dict0.keys() | dict1.keys() | dict2.keys():\n",
    "    for d in (dict0, dict1, dict2):\n",
    "        if d.__contains__(key):\n",
    "            cadillac_id_labels[key] = d[key]\n",
    "            break\n",
    "cadillac_id_lsb = dict()\n",
    "for key in all_id_lsbs0.keys() | all_id_lsbs1.keys() | all_id_lsbs2.keys():\n",
    "    for d in (all_id_lsbs0, all_id_lsbs1, all_id_lsbs2):\n",
    "        if d.__contains__(key):\n",
    "            cadillac_id_lsb[key] = d[key]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f683a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\cadillac_ct6_object.dbc\n",
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\cadillac_ct6_powertrain.dbc\n",
      "D:\\--storage--\\program\\Code\\jupyter-notebook\\CarNetworkIDS\\Code\\MyTask\\My_DBC\\gm_global_a_powertrain.dbc\n"
     ]
    }
   ],
   "source": [
    "# 合并所有Cadillac以及Chevy车的Ground Truth的 0/1 label\n",
    "all_id_lsbs0, dict0 = get_id_labels(0)\n",
    "all_id_lsbs1, dict1 = get_id_labels(1)\n",
    "all_id_lsbs2, dict2 = get_id_labels(2)\n",
    "chevy_id_labels = dict()\n",
    "for key in dict0.keys() | dict1.keys() | dict2.keys():\n",
    "    for d in (dict0, dict1, dict2):\n",
    "        if d.__contains__(key):\n",
    "            chevy_id_labels[key] = d[key]\n",
    "            break\n",
    "chevy_id_lsb = dict()\n",
    "for key in all_id_lsbs0.keys() | all_id_lsbs1.keys() | all_id_lsbs2.keys():\n",
    "    for d in (all_id_lsbs0, all_id_lsbs1, all_id_lsbs2):\n",
    "        if d.__contains__(key):\n",
    "            chevy_id_lsb[key] = d[key]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ab46a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plt_data_flip_dict(train_data_dict, train_label_dict):\n",
    "    for id in train_data_dict.keys():\n",
    "        messagelist = train_data_dict[id]\n",
    "        trace_len = len(messagelist)\n",
    "        bit_flip = np.zeros(64)\n",
    "        previous = messagelist[0]\n",
    "        for item in messagelist:\n",
    "            for ix in range(64):\n",
    "                if item[ix] != previous[ix]:\n",
    "                    bit_flip[ix] = bit_flip[ix] + 1\n",
    "            previous = item\n",
    "        for ix in range(64):\n",
    "            bit_flip[ix] = bit_flip[ix] / trace_len\n",
    "    #     print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "    #     print(id+\": \")\n",
    "        fig = plt.figure(figsize=(8,3))\n",
    "        plt.bar(np.arange(64),bit_flip)\n",
    "        plt.bar(np.arange(64),train_label_dict[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a095b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data_label_dict(true_train_ids, train_data_dict,train_label_dict):\n",
    "    new_train_data_dict = {}\n",
    "    new_train_label_dict = {}\n",
    "    for i in range(len(train_data_dict)):\n",
    "        if i in true_train_ids:\n",
    "            id = list(train_data_dict.keys())[i]\n",
    "            new_train_data_dict[id] = train_data_dict[id]\n",
    "            new_train_label_dict[id] = train_label_dict[id]\n",
    "    return new_train_data_dict, new_train_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad19b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_ids(true_train_ids, train_data_dict):\n",
    "    true_ids = []\n",
    "    for i in range(len(train_data_dict)):\n",
    "        if i in true_train_ids:\n",
    "            id = list(train_data_dict.keys())[i]\n",
    "            true_ids.append(id)\n",
    "    return true_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39cefd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_data(id_tracedict_int, all_id_labels): \n",
    "    cur_trace_data = []\n",
    "    cur_trace_label = []\n",
    "    for id in id_tracedict_int.keys():\n",
    "        cur_trace = id_tracedict_int[id]\n",
    "        cur_label = all_id_labels[id]\n",
    "        length = len(cur_trace)\n",
    "        start = 4000\n",
    "        end = start + 1000\n",
    "        n = int((length-4000)/1000)\n",
    "        for i in range(n):\n",
    "            cur_trace_data.append(cur_trace[start: end])\n",
    "            cur_trace_label.append(cur_label)\n",
    "            start = end\n",
    "            end = start + 1000\n",
    "    return cur_trace_data, cur_trace_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4c775",
   "metadata": {},
   "source": [
    "#### honda数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae72d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到Honda车的子数据集数据\n",
    "data_hd = pd.read_csv('../Data/Honda_process_bin/honda001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f178ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到Honda车子的数据集的大Trace dict\n",
    "id_tracedict_hd = get_trace(data_hd)\n",
    "honda_ids = get_ids(id_tracedict_hd, honda_id_labels)\n",
    "honda_data_dict = {}\n",
    "honda_label_dict = {}\n",
    "get_trace_data_dict(honda_ids, id_tracedict_hd, honda_id_labels, honda_data_dict, honda_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49f5b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_honda_ids = [1,6,7,10,12, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4405a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['158', '1d0', '1ea', '255', '309', '324']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_honda_idname = get_true_ids(true_honda_ids, honda_data_dict)\n",
    "true_honda_idname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a024cdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    344\n",
       "1    464\n",
       "2    490\n",
       "3    597\n",
       "4    777\n",
       "5    804\n",
       "Name: wo, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = pd.DataFrame(['158', '1d0', '1ea', '255', '309', '324'],columns=[\"wo\"])\n",
    "arr['wo'].apply(lambda x: int(x, 16))\n",
    "# arr\n",
    "#   (6 0+)  (5 0+)   (2 0- 2 0+)  (9 0+) (7 0+) (7 0+) (3 0+)\n",
    "# ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1fe3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_honda_data_dict, new_honda_label_dict = clear_data_label_dict(true_honda_ids, honda_data_dict, honda_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13e94bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "honda_trace_data, honda_trace_label = get_trace_data(new_honda_data_dict, new_honda_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048476d",
   "metadata": {},
   "source": [
    "#### cadillac数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25540b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到Cadillac车的子数据集数据\n",
    "data_cd = pd.read_csv('../Data/Cadillac_process_bin/001.txt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12719425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到Honda车子的数据集的大Trace dict\n",
    "id_tracedict_cd = get_trace(data_cd)\n",
    "cadillac_ids = get_ids(id_tracedict_cd, cadillac_id_labels)\n",
    "cadillac_data_dict = {}\n",
    "cadillac_label_dict = {}\n",
    "get_trace_data_dict(cadillac_ids, id_tracedict_cd, cadillac_id_labels, cadillac_data_dict, cadillac_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bb999f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cadillac_ids = [8,9,10,11,15,19,21,26,27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55352c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17d', '180', '184', '1a1', '1e5', '260', '262', '348', '34a']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_cadillac_idname = get_true_ids(true_cadillac_ids, cadillac_data_dict)\n",
    "true_cadillac_idname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67a89999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    381\n",
       "1    384\n",
       "2    388\n",
       "3    417\n",
       "4    485\n",
       "5    608\n",
       "6    610\n",
       "7    840\n",
       "8    842\n",
       "Name: wo, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = pd.DataFrame(['17d', '180', '184', '1a1', '1e5', '260', '262', '348', '34a'],columns=[\"wo\"])\n",
    "arr['wo'].apply(lambda x: int(x, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b05be3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cadillac_data_dict, new_cadillac_label_dict = clear_data_label_dict(true_cadillac_ids, cadillac_data_dict, cadillac_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd0181e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadillac_trace_data, cadillac_trace_label = get_trace_data(new_cadillac_data_dict, new_cadillac_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0586f5f",
   "metadata": {},
   "source": [
    "#### chevy数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "180743fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到Chevy车的子数据集数据\n",
    "data_ch = pd.read_csv('../Data/Chevy_process_bin/chevy001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e24cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tracedict_ch = get_trace(data_ch)\n",
    "chevy_ids = get_ids(id_tracedict_ch, chevy_id_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf5e048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chevy_data_dict = {}\n",
    "chevy_label_dict = {}\n",
    "get_trace_data_dict(chevy_ids, id_tracedict_ch, chevy_id_labels, chevy_data_dict, chevy_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b8da496",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_chevy_ids = [6,7,13,14,15,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f2f9fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17d', '1a1', '32a', '348', '34a', 'be']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_chevy_idname = get_true_ids(true_chevy_ids, chevy_data_dict)\n",
    "true_chevy_idname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59413683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    381\n",
       "1    417\n",
       "2    810\n",
       "3    840\n",
       "4    842\n",
       "5    190\n",
       "Name: wo, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = pd.DataFrame(['17d', '1a1', '32a', '348', '34a', 'be'],columns=[\"wo\"])\n",
    "arr['wo'].apply(lambda x: int(x, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adbc422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_checy_data_dict, new_chevy_label_dict = clear_data_label_dict(true_chevy_ids, chevy_data_dict, chevy_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55dbbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chevy_trace_data, chevy_trace_label = get_trace_data(new_checy_data_dict, new_chevy_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4e74d",
   "metadata": {},
   "source": [
    "#### 得到训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c1ec6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_label(id_tracedict_int, all_id_labels):\n",
    "    cur_trace_data, cur_trace_label = get_trace_data(id_tracedict_int, all_id_labels)\n",
    "    trace_indexs = pd.DataFrame(cur_trace_label).index.tolist()\n",
    "    random.shuffle(trace_indexs)\n",
    "    trace_data, trace_labels = np.array(cur_trace_data)[trace_indexs], np.array(cur_trace_label)[trace_indexs]\n",
    "    return trace_data, trace_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "098cdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data_label(cur_trace_data, cur_trace_label):\n",
    "    trace_indexs = pd.DataFrame(cur_trace_label).index.tolist()\n",
    "    random.shuffle(trace_indexs)\n",
    "    trace_data, trace_labels = np.array(cur_trace_data)[trace_indexs], np.array(cur_trace_label)[trace_indexs]\n",
    "    return trace_data, trace_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a14ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate([honda_trace_data,cadillac_trace_data])\n",
    "train_labels = np.concatenate([honda_trace_label,cadillac_trace_label])\n",
    "train_data, train_labels = shuffle_data_label(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab003f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = shuffle_data_label(chevy_trace_data, chevy_trace_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e03d2",
   "metadata": {},
   "source": [
    "## DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "131798c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetor2matrix(cur_message):\n",
    "    cur_matrix = [[0] * 8 for _ in range(8)]\n",
    "    index = 0\n",
    "    for row in range(8):\n",
    "        for col in range(8):\n",
    "            cur_matrix[row][col] = cur_message[index] \n",
    "            index = index + 1\n",
    "    return cur_matrix\n",
    "\n",
    "def trace2matrixTrace(trace_data, trace_label):\n",
    "    matrix_data = []\n",
    "    matrix_label = []\n",
    "    for i in range(len(trace_data)):\n",
    "        cur_trace_data = trace_data[i]\n",
    "        cur_matrix_data = []\n",
    "        for message in cur_trace_data:\n",
    "            cur_matrix = vetor2matrix(message)\n",
    "            cur_matrix_data.append(cur_matrix)\n",
    "        matrix_data.append(np.concatenate(cur_matrix_data))\n",
    "        matrix_label.append(vetor2matrix(trace_label[i]))\n",
    "    return np.array(matrix_data), np.array(matrix_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85a337c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_train_data, matrix_train_labels = trace2matrixTrace(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b5aa498",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_test_data, matrix_test_labels = trace2matrixTrace(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2075a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\--storage--\\program\\application\\windows-installer\\Miniconda3\\Miniconda3\\envs\\keras_py3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\--storage--\\program\\application\\windows-installer\\Miniconda3\\Miniconda3\\envs\\keras_py3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\--storage--\\program\\application\\windows-installer\\Miniconda3\\Miniconda3\\envs\\keras_py3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\--storage--\\program\\application\\windows-installer\\Miniconda3\\Miniconda3\\envs\\keras_py3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\--storage--\\program\\application\\windows-installer\\Miniconda3\\Miniconda3\\envs\\keras_py3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\--storage--\\program\\application\\windows-installer\\Miniconda3\\Miniconda3\\envs\\keras_py3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Input, Permute, BatchNormalization, Flatten, MaxPooling1D, Conv1D,Conv2D, MaxPooling2D\n",
    "from keras.layers import concatenate, add, Reshape\n",
    "from keras.losses import mean_absolute_percentage_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras.backend as K\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60cba220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45f1e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0d53168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model.h5')\n",
    "# model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68853e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_4_1 = np.expand_dims(matrix_train_data, 3)\n",
    "test_data_4_1 = np.expand_dims(matrix_test_data, 3)\n",
    "train_data_4_2 = np.expand_dims(train_data, 3)\n",
    "test_data_4_2 = np.expand_dims(test_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7e33430",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain_data = [train_data_4_1, train_data_4_2]\n",
    "mytrain_labels = np.array(train_labels)\n",
    "\n",
    "mytest_data = [test_data_4_1, test_data_4_2]\n",
    "mytest_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8896f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs1 = Input(shape = (8000,8,1))\n",
    "# inputs2 = Input(shape = (1000,64,1))\n",
    "\n",
    "# x1 = Conv2D(15, (3,3), padding='same', data_format='channels_last')(inputs1)\n",
    "# x1 = Dropout(0.1)(x1)\n",
    "\n",
    "# x2 = Conv2D(15, (8,8), padding='same', data_format='channels_last')(inputs1)\n",
    "# x2 = Dropout(0.1)(x2)\n",
    "\n",
    "# x12 = concatenate([x1,x2])\n",
    "# x12 = add([x12,inputs1])\n",
    "# x12 = Reshape((1000,8,8,30))(x12)\n",
    "# x12 = layers.Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(x12)\n",
    "# x12 = Reshape((8,8,30))(x12)\n",
    "\n",
    "# x3 = Conv2D(15, (3,3), padding='same', data_format='channels_last')(inputs2)\n",
    "# x3 = Dropout(0.1)(x3)\n",
    "# x3 = add([x3,inputs2])\n",
    "# x3 = layers.Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(x3)\n",
    "# x3 = Reshape((8,8,15))(x3)\n",
    "\n",
    "# x = concatenate([x12,x3])\n",
    "# x = Flatten()(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# outputs = Dense(64, activation='sigmoid')(x)\n",
    "# model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "# keras.optimizers.Adam(lr=1e-3)\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "# model.summary()\n",
    "# plot_model(model,to_file=\"multihead model_((1+2)+input1)+(3+input2).png\",show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d448198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 8000, 8, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8000, 8, 15)  150         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8000, 8, 15)  975         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000, 64, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8000, 8, 15)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8000, 8, 15)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1000, 64, 15) 150         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8000, 8, 15)  0           dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8000, 8, 15)  0           dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000, 64, 15) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1000, 8, 8, 1 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1000, 8, 8, 1 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1000, 64, 15) 0           dropout_3[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 8, 8, 15)  0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 8, 8, 15)  0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 64, 15)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 8, 8, 15)     0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 8, 8, 15)     0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 8, 8, 15)     0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 45)     0           reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2880)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2880)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2880)         11520       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           184384      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 201,339\n",
      "Trainable params: 195,579\n",
      "Non-trainable params: 5,760\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape = (8000,8,1))\n",
    "inputs2 = Input(shape = (1000,64,1))\n",
    "\n",
    "x1 = Conv2D(15, (3,3), padding='same', data_format='channels_last')(inputs1)\n",
    "# x1 = Conv2D(15, (3,3), padding='same', data_format='channels_last')(x1)\n",
    "x1 = Dropout(0.1)(x1)\n",
    "x1 = add([x1,inputs1])\n",
    "x1 = Reshape((1000,8,8,15))(x1)\n",
    "x1 = layers.Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(x1)\n",
    "x1 = Reshape((8,8,15))(x1)\n",
    "\n",
    "x2 = Conv2D(15, (8,8), padding='same', data_format='channels_last')(inputs1)\n",
    "# x2 = Conv2D(15, (8,8), padding='same', data_format='channels_last')(x2)\n",
    "x2 = Dropout(0.1)(x2)\n",
    "x2 = add([x2,inputs1])\n",
    "x2 = Reshape((1000,8,8,15))(x2)\n",
    "x2 = layers.Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(x2)\n",
    "x2 = Reshape((8,8,15))(x2)\n",
    "\n",
    "x3 = Conv2D(15, (3,3), padding='same', data_format='channels_last')(inputs2)\n",
    "# x3 = Conv2D(15, (3,3), padding='same', data_format='channels_last')(x3)\n",
    "x3 = Dropout(0.1)(x3)\n",
    "x3 = add([x3,inputs2])\n",
    "x3 = layers.Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(x3)\n",
    "x3 = Reshape((8,8,15))(x3)\n",
    "\n",
    "x = concatenate([x1,x2,x3])\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(64, activation='sigmoid')(x)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "421757fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"multihead model_((1_2+input1)+(2_2+input1)+(3_2+input2))\"\n",
    "model_name = \"multihead model_((1+input1)+(2+input1)+(3+input2))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fdf86e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,to_file=(model_name+\".png\"),show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fd1d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### vector model: 3*3 Epoch=16, 0.9808, 0.65\n",
    "##### matrix model: 8*8 Epoch=13, 0.9745, 0.58\n",
    "##### matrix model: 3*3 Epoch=12, 0.9810, 0.66\n",
    "##### 0.65和0.66相差很小，一个可能原因是因为，目前使用的dbc文件中全都是大端信号，无小端信号数量，所以两个模型效果相差不大。\n",
    "##### Honda cadilla和chevy车的DBC文件中会有少量的小端字节序信号，但是所筛选出的特征明显ID的信号都是大端字节序\n",
    "##### 但是试验表明，Matrix比vetor收敛更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "099b1a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285, 8000, 8, 1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### multihead model: (1+2)+3  Epoch=9 0.9698 0.55\n",
    "##### multihead model: ((1+2)+input1)+(3+input2)  Epoch=8 0.9801 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "179b48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStop = EarlyStopping(monitor='val_accuracy', patience=0,verbose=1, mode='max')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint((model_name+\".h5\"), \n",
    "#                             monitor='val_loss',\n",
    "                             monitor='val_acc',\n",
    "                            verbose=1, \n",
    "                            save_best_only=True, \n",
    "                            save_weights_only=False, \n",
    "#                             mode='min', \n",
    "                            mode='max', \n",
    "                            period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "674a1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 285 samples, validate on 87 samples\n",
      "Epoch 1/30\n",
      "285/285 [==============================] - 32s 113ms/step - loss: 0.7033 - acc: 0.5952 - val_loss: 0.6299 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64960, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 2/30\n",
      "285/285 [==============================] - 26s 91ms/step - loss: 0.4648 - acc: 0.7734 - val_loss: 0.4614 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.64960 to 0.80388, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 3/30\n",
      "285/285 [==============================] - 24s 83ms/step - loss: 0.2664 - acc: 0.8900 - val_loss: 0.2869 - val_acc: 0.8946\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.80388 to 0.89458, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 4/30\n",
      "285/285 [==============================] - 25s 87ms/step - loss: 0.1299 - acc: 0.9539 - val_loss: 0.2115 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.89458 to 0.93804, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 5/30\n",
      "285/285 [==============================] - 24s 85ms/step - loss: 0.0619 - acc: 0.9800 - val_loss: 0.1686 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.93804 to 0.96462, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 6/30\n",
      "285/285 [==============================] - 25s 88ms/step - loss: 0.0398 - acc: 0.9874 - val_loss: 0.1497 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.96462 to 0.96695, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 7/30\n",
      "285/285 [==============================] - 24s 85ms/step - loss: 0.0256 - acc: 0.9927 - val_loss: 0.1433 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.96695 to 0.97755, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 8/30\n",
      "285/285 [==============================] - 25s 88ms/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.1361 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.97755 to 0.97845, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 9/30\n",
      "285/285 [==============================] - 27s 94ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.1343 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.97845 to 0.97899, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 10/30\n",
      "285/285 [==============================] - 24s 83ms/step - loss: 0.0108 - acc: 0.9967 - val_loss: 0.1304 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.97899 to 0.97953, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 11/30\n",
      "285/285 [==============================] - 25s 88ms/step - loss: 0.0132 - acc: 0.9963 - val_loss: 0.1267 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.97953\n",
      "Epoch 12/30\n",
      "285/285 [==============================] - 24s 85ms/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.1212 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.97953\n",
      "Epoch 13/30\n",
      "285/285 [==============================] - 23s 82ms/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.1191 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.97953\n",
      "Epoch 14/30\n",
      "285/285 [==============================] - 23s 81ms/step - loss: 0.0077 - acc: 0.9980 - val_loss: 0.1173 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.97953 to 0.97971, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 15/30\n",
      "285/285 [==============================] - 24s 83ms/step - loss: 0.0060 - acc: 0.9985 - val_loss: 0.1134 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.97971\n",
      "Epoch 16/30\n",
      "285/285 [==============================] - 24s 85ms/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.1160 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.97971\n",
      "Epoch 17/30\n",
      "285/285 [==============================] - 24s 86ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.1222 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.97971 to 0.98042, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 18/30\n",
      "285/285 [==============================] - 25s 87ms/step - loss: 0.0058 - acc: 0.9984 - val_loss: 0.1269 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98042\n",
      "Epoch 19/30\n",
      "285/285 [==============================] - 24s 83ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.1229 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.98042 to 0.98078, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 20/30\n",
      "285/285 [==============================] - 23s 82ms/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.1205 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.98078 to 0.98132, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 21/30\n",
      "285/285 [==============================] - 23s 81ms/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.1177 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98132\n",
      "Epoch 22/30\n",
      "285/285 [==============================] - 23s 81ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.1163 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.98132 to 0.98150, saving model to multihead model_((1+input1)+(2+input1)+(3+input2)).h5\n",
      "Epoch 23/30\n",
      "285/285 [==============================] - 23s 82ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1122 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.98150\n",
      "Epoch 24/30\n",
      "285/285 [==============================] - 23s 82ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.1055 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.98150\n",
      "Epoch 25/30\n",
      "285/285 [==============================] - 24s 83ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.1103 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.98150\n",
      "Epoch 26/30\n",
      "285/285 [==============================] - 24s 83ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1111 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.98150\n",
      "Epoch 27/30\n",
      "285/285 [==============================] - 24s 84ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.1057 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.98150\n",
      "Epoch 28/30\n",
      "285/285 [==============================] - 26s 91ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1086 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.98150\n",
      "Epoch 29/30\n",
      "285/285 [==============================] - 24s 84ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1145 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.98150\n",
      "Epoch 30/30\n",
      "285/285 [==============================] - 24s 85ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.1150 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.98150\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(mytrain_data, mytrain_labels,\n",
    "     epochs=30,\n",
    "     batch_size=32,\n",
    "     callbacks=[checkpoint],\n",
    "     validation_data=(mytest_data, mytest_labels)\n",
    "    )\n",
    "# model.save('multihead model_((1+2)+input1)+(3+input2).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6465b831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# acc = history.history['acc']\n",
    "# loss = history.history['loss']\n",
    "# epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "634f8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Accuracy')\n",
    "# plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "# plt.plot(epochs, loss, 'blue', label='Training loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# 1CNN 8*8 0.58\n",
    "# 1CNN 3*3 12EPOCH 0.58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92157f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('multihead model_((1+2)+input1)+(3+input2).h5')\n",
    "best_model = load_model(model_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e39c7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集:\n",
      "预测正确数量,训练集样本量:\n",
      "5465 5568\n",
      "测试集精确度等指标：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      5417\n",
      "         1.0       0.65      0.68      0.66       151\n",
      "\n",
      "    accuracy                           0.98      5568\n",
      "   macro avg       0.82      0.83      0.83      5568\n",
      "weighted avg       0.98      0.98      0.98      5568\n",
      "\n",
      "混淆矩阵：\n",
      "[[5363   54]\n",
      " [  49  102]]\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集:\")\n",
    "predict_target_test_msb_prob =best_model.predict(mytest_data)\n",
    "predict_target_test_msb_label = (predict_target_test_msb_prob > 0.5).astype(int)\n",
    "predict_target_test_msb_1D = predict_target_test_msb_label.flatten()\n",
    "test_labels_1D = mytest_labels.flatten()\n",
    "print(\"预测正确数量,训练集样本量:\")\n",
    "print(sum(predict_target_test_msb_1D == test_labels_1D),len(test_labels_1D))\n",
    "print(\"测试集精确度等指标：\")\n",
    "print(metrics.classification_report(test_labels_1D,predict_target_test_msb_1D))\n",
    "print(\"混淆矩阵：\")\n",
    "print(metrics.confusion_matrix(test_labels_1D,predict_target_test_msb_1D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0112ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集:\n",
      "预测正确数量,训练集样本量:\n",
      "5359 5568\n",
      "测试集精确度等指标：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98      5417\n",
      "         1.0       0.39      0.68      0.49       151\n",
      "\n",
      "    accuracy                           0.96      5568\n",
      "   macro avg       0.69      0.82      0.74      5568\n",
      "weighted avg       0.97      0.96      0.97      5568\n",
      "\n",
      "混淆矩阵：\n",
      "[[5257  160]\n",
      " [  49  102]]\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集:\")\n",
    "predict_target_test_msb_prob = model.predict(mytest_data)\n",
    "predict_target_test_msb_label = (predict_target_test_msb_prob > 0.5).astype(int)\n",
    "predict_target_test_msb_1D = predict_target_test_msb_label.flatten()\n",
    "test_labels_1D = mytest_labels.flatten()\n",
    "print(\"预测正确数量,训练集样本量:\")\n",
    "print(sum(predict_target_test_msb_1D == test_labels_1D),len(test_labels_1D))\n",
    "print(\"测试集精确度等指标：\")\n",
    "print(metrics.classification_report(test_labels_1D,predict_target_test_msb_1D))\n",
    "print(\"混淆矩阵：\")\n",
    "print(metrics.confusion_matrix(test_labels_1D,predict_target_test_msb_1D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41858034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(model_name+'.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eb6fa3a81e2d7b30bb546db1db4686bad2879ee756f88b2ac00a1380e074e9e"
  },
  "kernelspec": {
   "display_name": "Python [conda env:keras_py3.7]",
   "language": "python",
   "name": "conda-env-keras_py3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
